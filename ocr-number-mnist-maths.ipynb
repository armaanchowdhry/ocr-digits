{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your image\n",
    "image = Image.open(\"Inputs/5.jpg\")  # Replace with the path to your image\n",
    "\n",
    "# Resize the image to 28x28 pixels\n",
    "image = image.resize((28, 28))\n",
    "\n",
    "# Convert to grayscale (if necessary)\n",
    "image = image.convert(\"L\")\n",
    "\n",
    "# Normalize pixel values\n",
    "image = np.array(image) / 255.0\n",
    "\n",
    "# Flatten the image to 1D array\n",
    "image = image.reshape(1, 784)  # Reshape into a single sample with 784 features\n",
    "\n",
    "# Now 'image' contains your input data in the MNIST format\n",
    "X_image = image.reshape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "SCALE_FACTOR = 255 # TRES IMPORTANT SINON OVERFLOW SUR EXP\n",
    "WIDTH = X_train.shape[1]\n",
    "HEIGHT = X_train.shape[2]\n",
    "WIDTH, HEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0],WIDTH*HEIGHT).T / SCALE_FACTOR\n",
    "X_test = X_test.reshape(X_test.shape[0],WIDTH*HEIGHT).T  / SCALE_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 60000), (784, 10000))"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    size , m = X_train.shape\n",
    "    nodes = 10\n",
    "    num_classes = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.rand(nodes,size) * np.sqrt(1./(size))\n",
    "b1 = np.random.rand(nodes,1) * np.sqrt(1./nodes)\n",
    "W2 = np.random.rand(nodes,nodes) * np.sqrt(1./nodes)\n",
    "b2 = np.random.rand(nodes,1) * np.sqrt(1./(size))\n",
    "W1.shape,b1.shape,W2.shape,b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 128\n",
    "\n",
    "# Calculate the total number of batches\n",
    "num_batches = len(X_train) // batch_size\n",
    "\n",
    "# Create mini-batches\n",
    "mini_batches = []\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = (i + 1) * batch_size\n",
    "    mini_batch_X = X_train[start_idx:end_idx]\n",
    "    mini_batch_Y = Y_train[start_idx:end_idx]\n",
    "    mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "\n",
    "# If there are any remaining data points, create one more mini-batch\n",
    "if len(X_train) % batch_size != 0:\n",
    "    mini_batch_X = X_train[num_batches * batch_size:]\n",
    "    mini_batch_Y = Y_train[num_batches * batch_size:]\n",
    "    mini_batches.append((mini_batch_X, mini_batch_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(Z,0)\n",
    "\n",
    "def derivative_ReLU(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    exp = np.exp(Z - np.max(Z)) #le np.max(Z) evite un overflow en diminuant le contenu de exp\n",
    "    return exp / exp.sum(axis=0)\n",
    "\n",
    "def init_params(nodes,size):\n",
    "    W1 = np.random.rand(nodes,size) * np.sqrt(1./(size))\n",
    "    b1 = np.random.rand(nodes,1) * np.sqrt(1./nodes)\n",
    "    W2 = np.random.rand(nodes,nodes) * np.sqrt(1./nodes)\n",
    "    b2 = np.random.rand(nodes,1) * np.sqrt(1./(size))\n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "def forward_propagation(X,W1,b1,W2,b2):\n",
    "    Z1 = W1.dot(X) + b1 #10, m\n",
    "    A1 = ReLU(Z1) # 10,m\n",
    "    Z2 = W2.dot(A1) + b2 #10,m\n",
    "    A2 = softmax(Z2) #10,m\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def one_hot(Y,nodes):\n",
    "    ''' return an 0 vector with 1 only in the position correspondind to the value in Y'''\n",
    "    one_hot_Y = np.zeros((nodes, Y.size))\n",
    "    one_hot_Y[Y, np.arange(Y.size)] = 1\n",
    "    return one_hot_Y\n",
    "\n",
    "def backward_propagation(nodes,X, Y, A1, A2, W2, Z1, m):\n",
    "    one_hot_Y = one_hot(Y,nodes)\n",
    "    dZ2 = 2*(A2 - one_hot_Y) #10,m\n",
    "    dW2 = 1/m * (dZ2.dot(A1.T)) # 10 , 10\n",
    "    db2 = 1/m * np.sum(dZ2,axis=1, keepdims=True) # 10, 1\n",
    "    dZ1 = W2.T.dot(dZ2)*derivative_ReLU(Z1) # 10, m\n",
    "    dW1 = 1/m * (dZ1.dot(X.T)) #10, 784\n",
    "    db1 = 1/m * np.sum(dZ1,axis=1, keepdims=True) # 10, 1\n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(nodes,alpha, W1, b1, W2, b2, dW1, db1, dW2, db2):\n",
    "    W1 -= alpha * dW1\n",
    "    b1 -= alpha * np.reshape(db1, (nodes,1))\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * np.reshape(db2, (nodes,1))\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y)/Y.size\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations,batch_size):\n",
    "    size , m = X.shape\n",
    "    nodes = 512\n",
    "\n",
    "    W1, b1, W2, b2 = init_params(nodes,size)\n",
    "    for i in range(iterations):\n",
    "        # Mini-batch training\n",
    "        for batch_start in range(0, m, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, m)\n",
    "            X_batch = X[:, batch_start:batch_end]\n",
    "            Y_batch = Y[batch_start:batch_end]\n",
    "            \n",
    "        Z1, A1, Z2, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "        dW1, db1, dW2, db2 = backward_propagation(nodes,X, Y, A1, A2, W2, Z1, m)\n",
    "\n",
    "        W1, b1, W2, b2 = update_params(nodes,alpha, W1, b1, W2, b2, dW1, db1, dW2, db2)   \n",
    "\n",
    "        if (i+1) % int(iterations/10) == 0:\n",
    "            print(f\"Iteration: {i+1} / {iterations}\")\n",
    "            prediction = get_predictions(A2)\n",
    "            print(f'{get_accuracy(prediction, Y):.3%}')\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def make_predictions(X, W1 ,b1, W2, b2):\n",
    "    _, _, _, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "    predictions = get_predictions(A2)\n",
    "    return predictions\n",
    "\n",
    "def show_prediction(index,X, Y, W1, b1, W2, b2):\n",
    "    # None => cree un nouvel axe de dimension 1, cela a pour effet de transposer X[:,index] qui un np.array de dimension 1 (ligne) et qui devient un vecteur (colonne)\n",
    "    #  ce qui correspond bien a ce qui est demande par make_predictions qui attend une matrice dont les colonnes sont les pixels de l'image, la on donne une seule colonne\n",
    "    vect_X = X[:, index,None]\n",
    "    prediction = make_predictions(vect_X, W1, b1, W2, b2)\n",
    "    label = Y[index]\n",
    "    print(\"Prediction: \", prediction)\n",
    "    print(\"Label: \", label)\n",
    "\n",
    "    current_image = vect_X.reshape((WIDTH, HEIGHT)) * SCALE_FACTOR\n",
    "\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.15, 200,batch_size)\n",
    "with open(\"trained_params.pkl\",\"wb\") as dump_file:\n",
    "    pickle.dump((W1, b1, W2, b2),dump_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trained_params.pkl\",\"rb\") as dump_file:\n",
    "    W1, b1, W2, b2=pickle.load(dump_file)\n",
    "show_prediction(0,X_test, Y_test, W1, b1, W2, b2)\n",
    "show_prediction(1,X_test, Y_test, W1, b1, W2, b2)\n",
    "show_prediction(2,X_test, Y_test, W1, b1, W2, b2)\n",
    "show_prediction(100,X_test, Y_test, W1, b1, W2, b2)\n",
    "show_prediction(200,X_test, Y_test, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predictions = make_predictions(X_test, W1, b1, W2, b2)\n",
    "accuracy = get_accuracy(dev_predictions, Y_test)\n",
    "print(f\"Accuracy on the test dataset: {accuracy:.3%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
